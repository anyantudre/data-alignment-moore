{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msilence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_on_silence\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "from huggingface_hub import login \n",
    "\n",
    "nltk.download('punkt')  # tokenizer for sentence splitting\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_on_silence(audio, silence_thresh=-50, min_silence_len=500, output_folder=\"segments\"):\n",
    "    # Split the audio based on silence\n",
    "    audio_chunks = split_on_silence(\n",
    "        audio, \n",
    "        min_silence_len=min_silence_len,  # Minimum length of silence (in ms)\n",
    "        silence_thresh=silence_thresh     # Silence threshold (in dBFS)\n",
    "    )\n",
    "    \n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save each audio segment and store their file paths\n",
    "    segment_paths = []\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "        segment_name = f\"segment_{i}.mp3\"\n",
    "        segment_path = os.path.join(output_folder, segment_name)\n",
    "        chunk.export(segment_path, format=\"mp3\")\n",
    "        segment_paths.append(segment_path)\n",
    "    \n",
    "    return segment_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_row(row, silence_thresh, min_silence_len, output_folder):\n",
    "    \"\"\"\n",
    "    Processes a single row of the dataset by segmenting its audio based on silence,\n",
    "    and splitting its transcription by sentences.\n",
    "    \n",
    "    Args:\n",
    "        row (dict): A single row of the dataset, containing 'audio' and 'transcription' fields.\n",
    "        silence_thresh (int): Silence threshold in dBFS.\n",
    "        min_silence_len (int): Minimum length of silence in milliseconds.\n",
    "        output_folder (str): Folder to store the segmented audio files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with segmented audio paths and corresponding transcription parts.\n",
    "    \"\"\"\n",
    "    audio_data = row['audio']['array']  # Get the raw audio array\n",
    "    sample_rate = row['audio']['sampling_rate']  # Get the sample rate\n",
    "\n",
    "    #print(f\"==================DEBUGGING===================> array size {audio_data.shape}\")\n",
    "    #print(f\"==================DEBUGGING===================> array type {audio_data.dtype}\")\n",
    "    #print(f\"==================DEBUGGING===================> array {audio_data}\")\n",
    "\n",
    "    # Check audio data shape\n",
    "    if len(audio_data.shape) == 1:  # Mono\n",
    "        channels = 1\n",
    "    elif len(audio_data.shape) == 2:  # Stereo\n",
    "        channels = audio_data.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported audio format\")\n",
    "\n",
    "    # If the audio data is in float format, normalize it and convert to int16\n",
    "    #if audio_data.dtype == np.float32:\n",
    "    #    audio_data = np.int16(audio_data / np.max(np.abs(audio_data)) * 32767)\n",
    "\n",
    "    # Create an AudioSegment from the array and sample rate\n",
    "    audio_segment = AudioSegment(\n",
    "        audio_data.tobytes(), \n",
    "        frame_rate=sample_rate, \n",
    "        sample_width=2, \n",
    "        channels=channels  # Set channels based on audio data shape\n",
    "    )\n",
    "    \n",
    "    # Split the audio into segments based on silence\n",
    "    #print(f\"==================DEBUGGING===================> Splitting the audio into segments\")\n",
    "    audio_segments = split_audio_on_silence(audio_segment, silence_thresh, min_silence_len, output_folder)\n",
    "    \n",
    "    # Split the transcription into sentences\n",
    "    transcription_segments = nltk.tokenize.sent_tokenize(row[\"transcript\"])  # Split transcription into sentences\n",
    "    \n",
    "    # If the number of audio segments and transcription segments don't match, we can flag it for manual review\n",
    "    if len(audio_segments) != len(transcription_segments):\n",
    "        print(f\"Warning: Mismatch between number of audio segments and transcription segments for {row['audio']['path']}\")\n",
    "    \n",
    "    return {\n",
    "        \"audio_segments\": audio_segments,\n",
    "        \"segment_transcriptions\": transcription_segments\n",
    "    }\n",
    "\n",
    "\n",
    "def process_dataset(dataset, output_folder, silence_thresh, min_silence_len):\n",
    "    \"\"\"\n",
    "    Processes the entire dataset by segmenting all audio files based on silence.\n",
    "    \n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face dataset containing 'audio' and 'transcription' columns.\n",
    "        output_folder (str): Folder where segmented audio files will be saved.\n",
    "        silence_thresh (int): Silence threshold in dBFS.\n",
    "        min_silence_len (int): Minimum length of silence in milliseconds.\n",
    "    \n",
    "    Returns:\n",
    "        Dataset: A processed dataset with additional columns for segmented audio paths and corresponding transcriptions.\n",
    "    \"\"\"\n",
    "    return dataset.map(lambda row: process_row(row, silence_thresh, min_silence_len, output_folder))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to parse command-line arguments and run the audio segmentation process.\n",
    "    \"\"\"\n",
    "    # Command-line argument parsing\n",
    "    parser = argparse.ArgumentParser(description=\"Segment long audios based on silence and update Hugging Face dataset\")\n",
    "    \n",
    "    # Required arguments\n",
    "    parser.add_argument(\"--dataset_name\", type=str, required=True, help=\"Name of the Hugging Face dataset (e.g., username/dataset_name)\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, required=True, help=\"Hugging Face token for authentication\")\n",
    "    parser.add_argument(\"--output_folder\", type=str, required=True, help=\"Folder to save the segmented audio files\")\n",
    "    parser.add_argument(\"--output_dataset\", type=str, required=True, help=\"Path to save the processed dataset (JSON format)\")\n",
    "    \n",
    "    # Optional arguments for silence detection configuration\n",
    "    parser.add_argument(\"--silence_thresh\", type=int, default=-50, help=\"Silence threshold in dBFS (default: -50)\")\n",
    "    parser.add_argument(\"--min_silence_len\", type=int, default=500, help=\"Minimum silence length in milliseconds (default: 500)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Log in to Hugging Face using the provided token\n",
    "    login(args.hf_token)\n",
    "    \n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    print(f\"Loading dataset {args.dataset_name} from Hugging Face Hub...\")\n",
    "    dataset = load_dataset(args.dataset_name)\n",
    "    \n",
    "    # Process the dataset by applying silence-based segmentation to each audio file\n",
    "    print(\"Processing dataset and segmenting audio files...\")\n",
    "    processed_dataset = process_dataset(dataset[\"train\"], args.output_folder, args.silence_thresh, args.min_silence_len)\n",
    "    \n",
    "    # Save the processed dataset\n",
    "    print(f\"Saving processed dataset to {args.output_dataset}...\")\n",
    "    processed_dataset.to_json(args.output_dataset)\n",
    "    \n",
    "    print(\"Audio segmentation completed and dataset saved successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
